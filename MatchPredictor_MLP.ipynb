{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4WnfXTuW2sb"
   },
   "source": [
    "# MatchPredictor\n",
    "\n",
    "### A neural network which predicts the outcomes of Premier League football matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4uLPYGzW2sb"
   },
   "source": [
    "#### Importing Libraries & Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L3D867YUW2sb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n",
      "                 It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data \n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukm7le4QW2sc"
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "1k1mUJUiW2sc"
   },
   "outputs": [],
   "source": [
    "# Data HPs\n",
    "past = 3     # How many past games are taken into account for team form calculations\n",
    "infer_data_percent = 0.2\n",
    "test_data_percent = 0.5\n",
    "\n",
    "# Training HPs\n",
    "rseed = 3 \n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "num_epochs = 30\n",
    "\n",
    "# Model HPs\n",
    "input_size = 12\n",
    "layer_sizes = [32,32,3] # in_size = 12, fc1_size = 32, fc2_size = 32, fc3_size = 20, out_size = 3\n",
    "act=0                 # 0 = ReLU,    1 = TanH\n",
    "loss_fcn_toggle = 0   # 0 = MSELoss, 1 = BCELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Is0DNAr7W2sc"
   },
   "source": [
    "#### Importing Match Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fC0-M0RiW2sc"
   },
   "outputs": [],
   "source": [
    "# Imported CSVs become pandas' DataFrames\n",
    "\n",
    "data_20_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2019-to-2020-stats.csv\")\n",
    "data_19_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2018-to-2019-stats.csv\")\n",
    "data_18_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2017-to-2018-stats.csv\")\n",
    "data_17_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2016-to-2017-stats.csv\")\n",
    "data_16_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2015-to-2016-stats.csv\")\n",
    "data_15_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2014-to-2015-stats.csv\")\n",
    "data_14_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2013-to-2014-stats.csv\")\n",
    "data_13_prem = pd.read_csv(\"Prem League Match Data\\england-premier-league-matches-2012-to-2013-stats.csv\")\n",
    "\n",
    "data_20_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2019-to-2020-stats.csv\")\n",
    "data_19_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2018-to-2019-stats.csv\")\n",
    "data_18_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2017-to-2018-stats.csv\")\n",
    "data_17_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2016-to-2017-stats.csv\")\n",
    "data_16_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2015-to-2016-stats.csv\")\n",
    "data_15_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2014-to-2015-stats.csv\")\n",
    "data_14_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2013-to-2014-stats.csv\")\n",
    "data_13_spain = pd.read_csv(\"La-Liga Match Data\\spain-la-liga-matches-2012-to-2013-stats.csv\")\n",
    "\n",
    "data_20_france = pd.read_csv(\"france-ligue-1-matches-2019-to-2020-stats.csv\")\n",
    "data_19_france = pd.read_csv(\"france-ligue-1-matches-2018-to-2019-stats.csv\")\n",
    "data_18_france = pd.read_csv(\"france-ligue-1-matches-2017-to-2018-stats.csv\")\n",
    "data_17_france = pd.read_csv(\"france-ligue-1-matches-2016-to-2017-stats.csv\")\n",
    "data_16_france = pd.read_csv(\"france-ligue-1-matches-2015-to-2016-stats.csv\")\n",
    "data_15_france = pd.read_csv(\"france-ligue-1-matches-2014-to-2015-stats.csv\")\n",
    "data_14_france = pd.read_csv(\"france-ligue-1-matches-2013-to-2014-stats.csv\")\n",
    "\n",
    "data_20_italy = pd.read_csv(\"Serie-A Match Data\\italy-serie-a-matches-2019-to-2020-stats.csv\")\n",
    "data_19_italy = pd.read_csv(\"Serie-A Match Data\\italy-serie-a-matches-2018-to-2019-stats.csv\")\n",
    "data_18_italy = pd.read_csv(\"Serie-A Match Data\\italy-serie-a-matches-2017-to-2018-stats.csv\")\n",
    "data_17_italy = pd.read_csv(\"Serie-A Match Data\\italy-serie-a-matches-2016-to-2017-stats.csv\")\n",
    "data_16_italy = pd.read_csv(\"Serie-A Match Data\\italy-serie-a-matches-2015-to-2016-stats.csv\")\n",
    "data_15_italy = pd.read_csv(\"Serie-A Match Data\\italy-serie-a-matches-2014-to-2015-stats.csv\")\n",
    "data_14_italy = pd.read_csv(\"Serie-A Match Data\\italy-serie-a-matches-2013-to-2014-stats.csv\")\n",
    "\n",
    "data_20_germany = pd.read_csv(\"Bundesliga Match Data\\germany-bundesliga-matches-2019-to-2020-stats.csv\")\n",
    "data_19_germany = pd.read_csv(\"Bundesliga Match Data\\germany-bundesliga-matches-2018-to-2019-stats.csv\")\n",
    "data_18_germany = pd.read_csv(\"Bundesliga Match Data\\germany-bundesliga-matches-2017-to-2018-stats.csv\")\n",
    "data_17_germany = pd.read_csv(\"Bundesliga Match Data\\germany-bundesliga-matches-2016-to-2017-stats.csv\")\n",
    "data_16_germany = pd.read_csv(\"Bundesliga Match Data\\germany-bundesliga-matches-2015-to-2016-stats.csv\")\n",
    "data_15_germany = pd.read_csv(\"Bundesliga Match Data\\germany-bundesliga-matches-2014-to-2015-stats.csv\")\n",
    "data_14_germany = pd.read_csv(\"Bundesliga Match Data\\germany-bundesliga-matches-2013-to-2014-stats.csv\")\n",
    "\n",
    "\n",
    "data = [data_20_prem, data_19_prem, data_18_prem, data_17_prem, data_16_prem, data_15_prem, data_14_prem, data_13_prem,\n",
    "        data_20_spain, data_19_spain, data_18_spain, data_17_spain, data_16_spain, data_15_spain, data_14_spain, data_13_spain,\n",
    "        data_20_france, data_19_france, data_18_france, data_17_france, data_16_france, data_15_france, data_14_france, \n",
    "        data_20_italy, data_19_italy, data_18_italy, data_17_italy, data_16_italy, data_15_italy, data_14_italy,\n",
    "        data_20_germany, data_19_germany, data_18_germany, data_17_germany, data_16_germany, data_15_germany, data_14_germany]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_1ZIfpIW2sc"
   },
   "source": [
    "#### DATA PREPROCSESSING  \n",
    "  \n",
    "Here we create the inputs for our model from the raw .csv files we collected from *footystats.org*.  \n",
    "  \n",
    "- Each season will be represented by a matrix.  \n",
    "- Each row of this matrix will consist of the inputs that describe a single match to the neural net.  \n",
    "  \n",
    "In each row, the entries are as follows:  \n",
    "*Note: **past** is an integer hyperparameter*  \n",
    "  \n",
    "**Index 0**: Home team average goals scored per game over last **past** games.  \n",
    "**Index 1**:  Home team average goals conceded per game over last **past** games.  \n",
    "**Index 2**:  Home team pre-match PPG.  \n",
    "**Index 3**:  Home team ppg from last game (so current game isn’t included).  \n",
    "**Index 4**:  Home team average number of shots on target over last **past** games.  \n",
    "**Index 5**:  Home team average number of corners over last **past** games.  \n",
    "**Index 6**:  Away team average goals scored per game over last **past** games.  \n",
    "**Index 7**:  Away team average goals conceded per game over last **past** games.  \n",
    "**Index 8**:  Away team pre-match PPG.  \n",
    "**Index 9**:  Away team ppg from last game (so current game isn’t included).  \n",
    "**Index 10**:  Away team average number of shots on target over last **past** games.  \n",
    "**Index 11**:  Away team average number of corners over last **past** games.  \n",
    "**Index 12 (LABEL)**:  0 if Home Team won, 1 if Away Team won, 2 if Draw. \n",
    "  \n",
    "  \n",
    "*NOTE: The first \"past\" weeks from each season cannot be used in training/testing as they have no previous matches to get data from.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Amz4WMIWW2sc",
    "outputId": "2fa7931f-8e83-40de-e21f-c5c07f3cda6d"
   },
   "outputs": [],
   "source": [
    "# Set up empty matrices to be filled with INPUTS & LABELS.\n",
    "# Once filled with values, each row will represent the inputs/label combo that describe a match to the NN.\n",
    "    \n",
    "prem_20 = np.zeros((380-(past*10),13))\n",
    "prem_19 = np.zeros((380-(past*10),13))\n",
    "prem_18 = np.zeros((380-(past*10),13))\n",
    "prem_17 = np.zeros((380-(past*10),13))\n",
    "prem_16 = np.zeros((380-(past*10),13))\n",
    "prem_15 = np.zeros((380-(past*10),13))\n",
    "prem_14 = np.zeros((380-(past*10),13))\n",
    "prem_13 = np.zeros((380-(past*10),13))\n",
    "\n",
    "spain_20 = np.zeros((380-(past*10),13))\n",
    "spain_19 = np.zeros((380-(past*10),13))\n",
    "spain_18 = np.zeros((380-(past*10),13))\n",
    "spain_17 = np.zeros((380-(past*10),13))\n",
    "spain_16 = np.zeros((380-(past*10),13))\n",
    "spain_15 = np.zeros((380-(past*10),13))\n",
    "spain_14 = np.zeros((380-(past*10),13))\n",
    "spain_13 = np.zeros((380-(past*10),13))\n",
    "\n",
    "france_20 = np.zeros((380-(past*10),13))\n",
    "france_19 = np.zeros((380-(past*10),13))\n",
    "france_18 = np.zeros((380-(past*10),13))\n",
    "france_17 = np.zeros((380-(past*10),13))\n",
    "france_16 = np.zeros((380-(past*10),13))\n",
    "france_15 = np.zeros((380-(past*10),13))\n",
    "france_14 = np.zeros((380-(past*10),13))\n",
    "\n",
    "italy_20 = np.zeros((380-(past*10),13))\n",
    "italy_19 = np.zeros((380-(past*10),13))\n",
    "italy_18 = np.zeros((380-(past*10),13))\n",
    "italy_17 = np.zeros((380-(past*10),13))\n",
    "italy_16 = np.zeros((380-(past*10),13))\n",
    "italy_15 = np.zeros((380-(past*10),13))\n",
    "italy_14 = np.zeros((380-(past*10),13))\n",
    "\n",
    "germany_20 = np.zeros((380-(past*10),13))\n",
    "germany_19 = np.zeros((380-(past*10),13))\n",
    "germany_18 = np.zeros((380-(past*10),13))\n",
    "germany_17 = np.zeros((380-(past*10),13))\n",
    "germany_16 = np.zeros((380-(past*10),13))\n",
    "germany_15 = np.zeros((380-(past*10),13))\n",
    "germany_14 = np.zeros((380-(past*10),13))\n",
    "\n",
    "\n",
    "input_seasons = [prem_20, prem_19, prem_18, prem_17, prem_16, prem_15, prem_14, prem_13,\n",
    "                 spain_20, spain_19, spain_18, spain_17, spain_16, spain_15, spain_14, spain_13,\n",
    "                 france_20, france_19, france_18, france_17, france_16, france_15, france_14, \n",
    "                 italy_20, italy_19, italy_18, italy_17, italy_16, italy_15, italy_14,\n",
    "                 germany_20, germany_19, germany_18, germany_17, germany_16, germany_15, germany_14]\n",
    "\n",
    "\n",
    "season_idx = 0\n",
    "for season in data:\n",
    "    \n",
    "    week = past + 1\n",
    "    input_season = input_seasons[season_idx]\n",
    "    \n",
    "    if season_idx < 30: # if any league other than German league...\n",
    "        weeks_in_season = 38\n",
    "        \n",
    "    if season_idx >= 30: # if German league...\n",
    "        weeks_in_season = 34\n",
    "    \n",
    "    while week <= weeks_in_season:\n",
    "        \n",
    "        if season_idx < 30:\n",
    "            match_count = 0                   # counting the 10 matches played in a given week\n",
    "            \n",
    "        if season_idx >= 30:\n",
    "            match_count = 1                   # **HACK: german league has only 9 matches in a given week**\n",
    "            \n",
    "        \n",
    "        row_idx = (week-1)*(10-match_count)             # index of the first match of new week\n",
    "\n",
    "        \n",
    "        while match_count < 10:\n",
    "    \n",
    "            match = season.iloc[[row_idx]]                     # saving current match\n",
    "            \n",
    "             # getting match LABEL (match result)\n",
    "            if (match.iat[0,12] > match.iat[0,13]):            \n",
    "                    result = 0   # home team win\n",
    "            elif (match.iat[0,12] < match.iat[0,13]):\n",
    "                    result = 1   # away team win\n",
    "            else:\n",
    "                    result = 2   # draw\n",
    "                    \n",
    "            \n",
    "            home_team = season.at[row_idx,'home_team_name']    # saving home team name\n",
    "            away_team = season.at[row_idx,'away_team_name']    # saving away team name\n",
    "            \n",
    "            total_home_goals = 0                               # counts total goals scored by home team over \"past\" matches\n",
    "            total_away_goals = 0                               # counts total goals scored by away team over \"past\" matches\n",
    "            \n",
    "            total_home_conceded = 0                            # counts total goals scored against home team over \"past\" matches \n",
    "            total_away_conceded = 0                            # counts total goals scored against away team over \"past\" matches\n",
    "            \n",
    "            total_home_shotson = 0                             # counts total shots on target taken by home team over...\n",
    "            total_away_shotson = 0                             # counts total shots on target taken by away team over...\n",
    "            \n",
    "            total_home_corners = 0                             # counts total corners taken by home team over...\n",
    "            total_away_corners = 0                             # counts total corners taken by away team over...\n",
    "            \n",
    "            home_PPG_pre = match.iat[0,8]                      # home team pre-match points per game (PPG)\n",
    "            away_PPG_pre = match.iat[0,9]                      # away team pre-match points per game (PPG)\n",
    "            \n",
    "            \n",
    "            previous = 1                                       # counts up to \"past\"\n",
    "            \n",
    "            \n",
    "            while previous <= past:\n",
    "\n",
    "                home_match_prev = season.loc[(season['home_team_name'] == home_team) & (season['Game Week'] == week-previous)]   # picking out home team's previous match\n",
    "                h_sc_idx = 12       # home team score index\n",
    "                h_shon_idx = 32     # home shots on target index\n",
    "                h_corn_idx = 20     # home corners index\n",
    "                h_ppg_idx = 10\n",
    "                \n",
    "                away_match_prev = season.loc[(season['away_team_name'] == away_team) & (season['Game Week'] == week-previous)]   # picking out away team's previous match\n",
    "                a_sc_idx = 13       # away team score index\n",
    "                a_shon_idx = 33     # away team shots on target index\n",
    "                a_corn_idx = 21     # away corners index\n",
    "                a_ppg_idx = 11\n",
    "                \n",
    "                # if home team name was not found in 'home_team_name' column...\n",
    "                if (home_match_prev.size == 0):\n",
    "                    home_match_prev = season.loc[(season['away_team_name'] == home_team) & (season['Game Week'] == week-previous)]   # picking out home team's previous match\n",
    "                    h_sc_idx = 13      # home team score index\n",
    "                    h_shon_idx = 33    # home shots on target index\n",
    "                    h_corn_idx = 21    # home corners index\n",
    "                    h_ppg_idx = 11\n",
    "                    \n",
    "                # if away team name was not found in 'away_team_name' column...\n",
    "                if (away_match_prev.size == 0):\n",
    "                    away_match_prev = season.loc[(season['home_team_name'] == away_team) & (season['Game Week'] == week-previous)]   # picking out away team's previous match\n",
    "                    a_sc_idx = 12      # away team score index\n",
    "                    a_shon_idx = 32    # away team shots on target index\n",
    "                    a_corn_idx = 20    # away corners index\n",
    "                    a_ppg_idx = 10\n",
    "                    \n",
    "                # if loop is 1 match in the past...   \n",
    "                if previous == 1:\n",
    "                    home_PPG = home_match_prev.iat[0,h_ppg_idx]\n",
    "                    away_PPG = away_match_prev.iat[0,a_ppg_idx]\n",
    "                    \n",
    "                #print(home_team, 'goals scored in week', week-previous, '= ', home_match_prev.iat[0,h_sc_idx])\n",
    "                total_home_goals += home_match_prev.iat[0,h_sc_idx]\n",
    "                total_away_goals += away_match_prev.iat[0,a_sc_idx]\n",
    "                \n",
    "                #print(home_team, 'goals conceded in week', week-previous, '=', home_match_prev.iat[0,a_sc_idx])\n",
    "                total_home_conceded += home_match_prev.iat[0,a_sc_idx]\n",
    "                total_away_conceded += away_match_prev.iat[0,h_sc_idx]\n",
    "                \n",
    "                total_home_shotson += home_match_prev.iat[0,h_shon_idx]\n",
    "                total_away_shotson += away_match_prev.iat[0,a_shon_idx]\n",
    "                \n",
    "                total_home_corners += home_match_prev.iat[0,h_corn_idx]\n",
    "                total_away_corners += away_match_prev.iat[0,a_corn_idx]\n",
    "                \n",
    "                \n",
    "                previous += 1\n",
    "\n",
    "            in_idx = row_idx - (past*10)\n",
    "            input_season[in_idx][0] = total_home_goals/past          # input INDEX 0 (home team avg. goals over \"past\")\n",
    "            input_season[in_idx][1] = total_home_conceded/past       # input INDEX 1 (home team avg. conceded goals over \"past\")\n",
    "            input_season[in_idx][2] = home_PPG_pre                   # input INDEX 2 (home team pre-match PPG: PPG in current season)\n",
    "            input_season[in_idx][3] = home_PPG                       # input INDEX 3 (home team PPG including past seasons)\n",
    "            input_season[in_idx][4] = total_home_shotson/past        # input INDEX 4 (home team avg. shots on target over \"past\")\n",
    "            input_season[in_idx][5] = total_home_corners/past        # input INDEX 5 (home team avg. corner kicks over \"past\")\n",
    "            input_season[in_idx][6] = total_away_goals/past          # input INDEX 6 (away team avg. goals over \"past\")\n",
    "            input_season[in_idx][7] = total_away_conceded/past       # input INDEX 7 (away team avg. conceded goals over \"past\")\n",
    "            input_season[in_idx][8] = away_PPG_pre                   # input INDEX 8 (away team pre-match PPG: PPG in current season)\n",
    "            input_season[in_idx][9] = away_PPG                       # input INDEX 9 (away team PPG including past seasons)\n",
    "            input_season[in_idx][10] = total_away_shotson/past       # input INDEX 10 (away team avg. shots on target over \"past\")\n",
    "            input_season[in_idx][11] = total_away_corners/past       # input INDEX 11 (away team avg. corner kicks over \"past\")\n",
    "            input_season[in_idx][12] = result                        # label INDEX 12 (match result)\n",
    "            \n",
    "            #print(input_season[in_idx])\n",
    "            \n",
    "            row_idx += 1\n",
    "            match_count += 1\n",
    "\n",
    "        week += 1\n",
    "        \n",
    "    season_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bNmJ7F2e0c8L",
    "outputId": "18fe3cc9-2615-4d29-c750-91a3a0905779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers of match outcomes (Home Team Wins, Away Team Wins, Draws): 0.0    6151\n",
      "1.0    3643\n",
      "2.0    3156\n",
      "Name: outcome, dtype: int64\n",
      "2.0    3156\n",
      "1.0    3156\n",
      "0.0    3156\n",
      "Name: outcome, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining all season input data into one dataset\n",
    "data_full_np = np.concatenate((input_seasons), axis=0)\n",
    "\n",
    "# Turn numpy array back into pd.DataFrame\n",
    "columns = ['h_goals','h_conceded','h_prePPG','h_avgPPG','h_shotsOn','h_corners',\n",
    "           'a_goals','a_conceded','a_prePPG','a_avgPPG','a_shotsOn','a_corners',\n",
    "           'outcome']\n",
    "\n",
    "data_full = pd.DataFrame(data_full_np, columns=columns)\n",
    "\n",
    "#Print 'outcome' column sums to determine dataset balance\n",
    "print('The numbers of match outcomes (Home Team Wins, Away Team Wins, Draws):', data_full[\"outcome\"].value_counts())\n",
    "\n",
    "#============================ BALANCE THE DATASET ==========================\n",
    "# draws are the under-represented class, so...\n",
    "# set home wins count = draws count\n",
    "# set away wins count = draws count\n",
    "\n",
    "data_full_home  = data_full[data_full[\"outcome\"]==0.0]\n",
    "data_full_away = data_full[data_full[\"outcome\"]==1.0]\n",
    "data_full_tie = data_full[data_full[\"outcome\"]==2.0]\n",
    "\n",
    "data_full_home = data_full_home.sample(len(data_full_tie), random_state=0)\n",
    "data_full_away = data_full_away.sample(len(data_full_tie), random_state=0)\n",
    "\n",
    "subsets = [data_full_home,data_full_away,data_full_tie]\n",
    "data_full = pd.concat(subsets)\n",
    "print(data_full[\"outcome\"].value_counts(),'\\n')\n",
    "\n",
    "#==========================================================================\n",
    "\n",
    "# Get a better idea of what our data looks like BEFORE NORMALIZATION\n",
    "def verbose_print(data):     # helper function\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(data.head())\n",
    "       \n",
    "# print(\"\\n \\n Data characteristics:\")\n",
    "# verbose_print(data_full.describe())\n",
    "\n",
    "# Turn 'outcome' column values into one hot-encoded form\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(data_full_np[:,12])   # y is vector of 1-hot encoded labels\n",
    "\n",
    "#y = data_full_np[:,12]\n",
    "#X = data_full_np[:,:12]                    # x is matrix of inputs (need to be normalized still)\n",
    "\n",
    "# Normalize continuous inputs\n",
    "X = data_full.drop(columns=['outcome'])\n",
    "\n",
    "\n",
    "for feature in X:\n",
    "  mean = X[feature].mean()\n",
    "  std = X[feature].std()\n",
    "  X[feature] = X[feature] - mean\n",
    "  X[feature] = X[feature]/std\n",
    "\n",
    "\n",
    "X = X.values\n",
    "X = train_test_split(X,test_size=infer_data_percent,random_state=1)\n",
    "y = train_test_split(y,test_size=infer_data_percent,random_state=1)\n",
    "\n",
    "X_train = X[0]  # extracting training inputs\n",
    "y_train = y[0]  # extracting training labels\n",
    "X_infer = train_test_split(X[1],test_size = test_data_percent,random_state=1) # splitting inputs into validation and test sets\n",
    "y_infer = train_test_split(y[1],test_size = test_data_percent,random_state=1) # splitting labels into validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_QroHKoW2sd"
   },
   "source": [
    "## Multi-Layer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "uEdgVrZRW2sd"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, layer_sizes, act):\n",
    "        \n",
    "        # Current parameters are:\n",
    "        # input_size = size of input data vector\n",
    "        # layer_sizes = list of each layer size, last layer size MUST = 3, ex: [32,32,20,3]\n",
    "        \n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        \n",
    "        # setting layer sizes\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.linears.append(nn.Linear(input_size,layer_sizes[0]))    # first layer created manually\n",
    "        for k in range(len(layer_sizes)-1):\n",
    "            self.linears.append(nn.Linear(layer_sizes[k], layer_sizes[k+1])) # creating all other layers\n",
    "            \n",
    "        # setting activation function\n",
    "        if act == 0:\n",
    "          self.act = nn.ReLU()\n",
    "        elif act == 1:  \n",
    "          self.act = nn.Tanh()\n",
    "        \n",
    "        self.out = nn.Softmax(dim=1)\n",
    "\n",
    "        ''' # manual method\n",
    "        self.fc2 = nn.Linear(32,32)\n",
    "        self.fc3 = nn.Linear(32,20)\n",
    "        self.fc4 = nn.Linear(20,3) \n",
    "        '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        layer_count = len(self.linears)\n",
    "        \n",
    "        for i in range(layer_count-1):\n",
    "            x = self.act(self.linears[i](x))\n",
    "        \n",
    "        x = self.out(self.linears[layer_count-1](x))\n",
    "      \n",
    "        ''' # manual method\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        '''\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK461FjGZrXc"
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE-dBefR_wCv"
   },
   "source": [
    "#### Dataloader creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "Bf7SrNHz_xq7"
   },
   "outputs": [],
   "source": [
    "# MatchDataset turns matrix-style datasets into map-style datasets\n",
    "class MatchDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index],self.y[index]\n",
    "    \n",
    "\n",
    "def load_data(batch_size):\n",
    "\n",
    "    train_dataset = MatchDataset(X_train,y_train)\n",
    "    valid_dataset = MatchDataset(X_infer[0],y_infer[0])\n",
    "    test_dataset = MatchDataset(X_infer[1],y_infer[1])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M0Mu74T_7V0"
   },
   "source": [
    "#### Get prediction from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "tnXEA5wlAAUu"
   },
   "outputs": [],
   "source": [
    "def get_preds(z):\n",
    "  \n",
    "  out = np.zeros(z.shape)\n",
    "  #inputs: output of model\n",
    "  #outputs: corresponding output to prediction\n",
    "  max_idxs = torch.max(z,1)[1]\n",
    "  for entry,idx in enumerate(max_idxs,0):\n",
    "      for val in range(3):\n",
    "        if val == idx:\n",
    "          out[entry][val] = 1\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ErVMQAJVfGi6"
   },
   "source": [
    "#### Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "5GGnhmzFZzpc"
   },
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    #inputs: preds: array, labels:array \n",
    "    #output: overall accuracy\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    '''   # for BCEwithLogitsLoss\n",
    "    for batch in range(len(preds)):\n",
    "        idx = labels[batch].int()\n",
    "        if (preds[batch][idx] == 1):\n",
    "            correct += 1\n",
    "    '''\n",
    "    \n",
    "      # FOR MSE Loss\n",
    "    for batch in range(len(preds)):\n",
    "        for i in range(3):\n",
    "            if (preds[batch][i] == 1) and (labels[batch][i] == 1):\n",
    "                correct += 1     \n",
    "    \n",
    "    \n",
    "    return (correct/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJlCum_EBNLh"
   },
   "source": [
    "#### Validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "H8PwjiNodBWz"
   },
   "outputs": [],
   "source": [
    "def validate(model,val_loader,loss_fcn):\n",
    "  val_acc=[]\n",
    "  val_loss=[]\n",
    "  for epoch in range(0,num_epochs):\n",
    "    for i,data in enumerate(val_loader,0): #iterate over val_loader, start idx=0\n",
    "      inputs,labels = data\n",
    "\n",
    "      z = model(inputs.float())  #z is size=3 \n",
    "\n",
    "      preds = get_preds(z) #preds used for accuracy\n",
    "    \n",
    "      #loss = loss_fcn(input=z, target=labels.long())  # for CrossEntropyLoss\n",
    "        \n",
    "      loss = loss_fcn(input=z, target=labels.float())  # FOR MSE LOSS\n",
    "\n",
    "      val_acc.append(accuracy(preds,labels))\n",
    "      val_loss.append(loss.item())\n",
    "       \n",
    "  ValLoss = sum(val_loss)/len(val_loss)\n",
    "  ValAcc = sum(val_acc) / len(val_acc)\n",
    "  \n",
    "  return ValAcc, ValLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader,loss_fcn):\n",
    "  test_acc=[]\n",
    "  test_loss=[]\n",
    "  for epoch in range(0,num_epochs):\n",
    "    for i,data in enumerate(test_loader,0): #iterate over val_loader, start idx=0\n",
    "      inputs,labels = data\n",
    "\n",
    "      z = model(inputs.float())  #z is size=3 \n",
    "\n",
    "      preds = get_preds(z) #preds used for accuracy\n",
    "        \n",
    "      loss = loss_fcn(input=z, target=labels.float())  # FOR MSE LOSS\n",
    "\n",
    "      test_acc.append(accuracy(preds,labels))\n",
    "      test_loss.append(loss.item())\n",
    "       \n",
    "  TestLoss = sum(test_loss)/len(test_loss)\n",
    "  TestAcc = sum(test_acc) / len(test_acc)\n",
    "  \n",
    "  return TestAcc, TestLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbQCrISXWN46"
   },
   "source": [
    "#### Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Sd9apCNOWOHQ"
   },
   "outputs": [],
   "source": [
    "def plot(epoch, train_var, val_var,xlabel,ylabel):\n",
    "    plt.plot(epoch, train_var, label = 'train')\n",
    "    plt.plot(epoch, val_var, label = 'validation')\n",
    "    plt.title(str(xlabel) + ' vs. '+ str(ylabel))\n",
    "    plt.xlabel(str(xlabel))\n",
    "    plt.ylabel(str(ylabel))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kAaa_MEjVrw"
   },
   "source": [
    "#### MLP loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "Q3vF0n-IjUwj"
   },
   "outputs": [],
   "source": [
    "def load_MLP(lr):\n",
    "    \n",
    "    #add optimizer,loss functions as a hyperparameters\n",
    "    model = MultiLayerPerceptron(input_size,layer_sizes,act)\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "\n",
    "    if loss_fcn_toggle == 0:\n",
    "        loss_fcn = nn.MSELoss()\n",
    "    if loss_fcn_toggle == 1:\n",
    "        loss_fcn = nn.BCELoss()\n",
    "    \n",
    "    return model,optimizer,loss_fcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfaT7X6ZiWn_"
   },
   "source": [
    "##Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "18YWlXsYim9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "train acc: 0.3625575373993095\n",
      "val acc: 0.45172514619883036\n",
      "train loss: 0.22270836074392503\n",
      "val loss: 0.21662455474336942\n",
      "final output: tensor([[0.4625, 0.3232, 0.2144],\n",
      "        [0.4518, 0.3288, 0.2194],\n",
      "        [0.4260, 0.3412, 0.2327],\n",
      "        [0.3849, 0.3435, 0.2716],\n",
      "        [0.4745, 0.2930, 0.2325],\n",
      "        [0.3795, 0.3778, 0.2427],\n",
      "        [0.4302, 0.2732, 0.2966],\n",
      "        [0.4310, 0.2642, 0.3047],\n",
      "        [0.4003, 0.3272, 0.2724],\n",
      "        [0.3608, 0.3612, 0.2780],\n",
      "        [0.4200, 0.2690, 0.3110],\n",
      "        [0.3472, 0.3234, 0.3294],\n",
      "        [0.3921, 0.2606, 0.3473],\n",
      "        [0.3907, 0.3528, 0.2564],\n",
      "        [0.4081, 0.2784, 0.3134],\n",
      "        [0.4231, 0.3098, 0.2672],\n",
      "        [0.4247, 0.3699, 0.2054],\n",
      "        [0.5209, 0.3117, 0.1674],\n",
      "        [0.4173, 0.2918, 0.2909],\n",
      "        [0.4928, 0.2585, 0.2488],\n",
      "        [0.3806, 0.3782, 0.2413],\n",
      "        [0.4282, 0.3448, 0.2270]], grad_fn=<SoftmaxBackward>)\n",
      "Epoch: 2\n",
      "train acc: 0.4673595128500192\n",
      "val acc: 0.4614565058479532\n",
      "train loss: 0.2149721948657861\n",
      "val loss: 0.21492637077967325\n",
      "final output: tensor([[0.5535, 0.2051, 0.2415],\n",
      "        [0.4292, 0.2981, 0.2727],\n",
      "        [0.4072, 0.2757, 0.3171],\n",
      "        [0.4341, 0.3173, 0.2485],\n",
      "        [0.4330, 0.3119, 0.2551],\n",
      "        [0.4694, 0.2941, 0.2365],\n",
      "        [0.4171, 0.2865, 0.2964],\n",
      "        [0.4730, 0.2744, 0.2526],\n",
      "        [0.5025, 0.2919, 0.2057],\n",
      "        [0.4423, 0.2946, 0.2630],\n",
      "        [0.4119, 0.3292, 0.2589],\n",
      "        [0.4517, 0.2800, 0.2683],\n",
      "        [0.4129, 0.3617, 0.2254],\n",
      "        [0.4392, 0.3035, 0.2573],\n",
      "        [0.4342, 0.3233, 0.2425],\n",
      "        [0.4988, 0.2639, 0.2373],\n",
      "        [0.5253, 0.3022, 0.1725],\n",
      "        [0.4345, 0.2823, 0.2831],\n",
      "        [0.4347, 0.2920, 0.2733],\n",
      "        [0.4163, 0.3315, 0.2522],\n",
      "        [0.6096, 0.2481, 0.1423],\n",
      "        [0.4522, 0.3120, 0.2358]], grad_fn=<SoftmaxBackward>)\n",
      "Epoch: 3\n",
      "train acc: 0.4725139048714998\n",
      "val acc: 0.46046235380116957\n",
      "train loss: 0.2138770116406654\n",
      "val loss: 0.2147285256617599\n",
      "final output: tensor([[0.4150, 0.3278, 0.2572],\n",
      "        [0.4579, 0.2919, 0.2502],\n",
      "        [0.4479, 0.3028, 0.2494],\n",
      "        [0.5254, 0.2605, 0.2141],\n",
      "        [0.4134, 0.2991, 0.2875],\n",
      "        [0.4265, 0.3061, 0.2675],\n",
      "        [0.5365, 0.2407, 0.2229],\n",
      "        [0.4184, 0.3086, 0.2730],\n",
      "        [0.5042, 0.2851, 0.2107],\n",
      "        [0.4812, 0.2836, 0.2352],\n",
      "        [0.4805, 0.2161, 0.3034],\n",
      "        [0.5010, 0.2686, 0.2305],\n",
      "        [0.4594, 0.3412, 0.1994],\n",
      "        [0.4192, 0.3201, 0.2608],\n",
      "        [0.5114, 0.2818, 0.2068],\n",
      "        [0.4773, 0.2570, 0.2657],\n",
      "        [0.4080, 0.3229, 0.2691],\n",
      "        [0.4775, 0.2675, 0.2551],\n",
      "        [0.5639, 0.2830, 0.1531],\n",
      "        [0.4208, 0.3140, 0.2652],\n",
      "        [0.4168, 0.3393, 0.2440],\n",
      "        [0.4220, 0.3341, 0.2439]], grad_fn=<SoftmaxBackward>)\n",
      "Epoch: 4\n",
      "train acc: 0.4730293440736479\n",
      "val acc: 0.46027229532163727\n",
      "train loss: 0.2135630098450536\n",
      "val loss: 0.21471738684508535\n",
      "final output: tensor([[0.4523, 0.2731, 0.2746],\n",
      "        [0.4219, 0.3350, 0.2431],\n",
      "        [0.4337, 0.3172, 0.2491],\n",
      "        [0.4068, 0.3151, 0.2781],\n",
      "        [0.4110, 0.3290, 0.2600],\n",
      "        [0.4700, 0.2589, 0.2711],\n",
      "        [0.4624, 0.2790, 0.2586],\n",
      "        [0.4640, 0.3212, 0.2148],\n",
      "        [0.5254, 0.2889, 0.1858],\n",
      "        [0.4889, 0.2652, 0.2459],\n",
      "        [0.4976, 0.2573, 0.2451],\n",
      "        [0.4730, 0.2997, 0.2273],\n",
      "        [0.4110, 0.3129, 0.2761],\n",
      "        [0.4906, 0.2373, 0.2721],\n",
      "        [0.4311, 0.3093, 0.2596],\n",
      "        [0.4208, 0.3252, 0.2540],\n",
      "        [0.4507, 0.2838, 0.2655],\n",
      "        [0.4813, 0.3017, 0.2170],\n",
      "        [0.4561, 0.2516, 0.2923],\n",
      "        [0.3986, 0.2878, 0.3136],\n",
      "        [0.4173, 0.2471, 0.3357],\n",
      "        [0.4151, 0.3039, 0.2810]], grad_fn=<SoftmaxBackward>)\n",
      "Epoch: 5\n",
      "train acc: 0.4729694092827004\n",
      "val acc: 0.4601297514619883\n",
      "train loss: 0.21335279036171828\n",
      "val loss: 0.21462694586979017\n",
      "final output: tensor([[0.4758, 0.3087, 0.2155],\n",
      "        [0.4715, 0.2664, 0.2622],\n",
      "        [0.4580, 0.3043, 0.2377],\n",
      "        [0.3987, 0.2802, 0.3211],\n",
      "        [0.4542, 0.3191, 0.2266],\n",
      "        [0.4261, 0.3045, 0.2695],\n",
      "        [0.4932, 0.2705, 0.2363],\n",
      "        [0.4708, 0.2916, 0.2376],\n",
      "        [0.6077, 0.2605, 0.1318],\n",
      "        [0.4128, 0.3202, 0.2670],\n",
      "        [0.4568, 0.3027, 0.2406],\n",
      "        [0.5495, 0.2309, 0.2195],\n",
      "        [0.4973, 0.2950, 0.2077],\n",
      "        [0.4141, 0.2850, 0.3009],\n",
      "        [0.5279, 0.2923, 0.1798],\n",
      "        [0.4637, 0.3165, 0.2198],\n",
      "        [0.4499, 0.3025, 0.2476],\n",
      "        [0.4565, 0.3142, 0.2293],\n",
      "        [0.5757, 0.2492, 0.1751],\n",
      "        [0.4942, 0.2543, 0.2515],\n",
      "        [0.5454, 0.2608, 0.1938],\n",
      "        [0.4899, 0.2747, 0.2354]], grad_fn=<SoftmaxBackward>)\n",
      "Epoch: 6\n",
      "train acc: 0.4731612006137323\n",
      "val acc: 0.4606286549707604\n",
      "train loss: 0.2132025725730864\n",
      "val loss: 0.2144487094051308\n",
      "final output: tensor([[0.5116, 0.2540, 0.2344],\n",
      "        [0.5027, 0.2655, 0.2319],\n",
      "        [0.4387, 0.3087, 0.2527],\n",
      "        [0.4176, 0.3011, 0.2813],\n",
      "        [0.4223, 0.3031, 0.2746],\n",
      "        [0.6696, 0.2016, 0.1288],\n",
      "        [0.4426, 0.2701, 0.2873],\n",
      "        [0.4271, 0.3069, 0.2660],\n",
      "        [0.4482, 0.2793, 0.2725],\n",
      "        [0.4402, 0.2877, 0.2722],\n",
      "        [0.4588, 0.2922, 0.2490],\n",
      "        [0.4711, 0.2819, 0.2470],\n",
      "        [0.4350, 0.2901, 0.2749],\n",
      "        [0.4699, 0.2997, 0.2304],\n",
      "        [0.4673, 0.2720, 0.2607],\n",
      "        [0.4498, 0.3214, 0.2287],\n",
      "        [0.4866, 0.2729, 0.2405],\n",
      "        [0.4760, 0.2739, 0.2501],\n",
      "        [0.4894, 0.2713, 0.2393],\n",
      "        [0.4558, 0.3124, 0.2317],\n",
      "        [0.4259, 0.3118, 0.2623],\n",
      "        [0.4529, 0.3203, 0.2268]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-7922e7075332>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-82-7922e7075332>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(rseed, lr, num_epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtrain_acc_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_loss_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#iterate over train_loader, start idx=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ece324\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ece324\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ece324\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\ece324\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ece324\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ece324\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ece324\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     61\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(rseed,lr,num_epochs):\n",
    "\n",
    "    torch.manual_seed(rseed)\n",
    "\n",
    "    model,optimizer,loss_fcn = load_MLP(lr)   #initialize model\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = load_data(batch_size) # get dataloaders\n",
    "\n",
    "    #records for plotting \n",
    "    TrainAccRec = []\n",
    "    TrainLossRec = []\n",
    "    ValAccRec = []  \n",
    "    ValLossRec = []\n",
    "    \n",
    "    tic = time()\n",
    " \n",
    "# ========================================TRAINING LOOP =========================================# \n",
    "    for epoch in range(0,num_epochs):\n",
    "        batch_count = 0\n",
    "        train_acc_sum = 0\n",
    "        train_loss_sum = 0\n",
    "        for i,data in enumerate(train_loader,0): #iterate over train_loader, start idx=0\n",
    "            inputs,labels = data\n",
    "\n",
    "            optimizer.zero_grad()  #initialize the gradients to zero  \n",
    "\n",
    "            z = model(inputs.float())  #z is size=3\n",
    "\n",
    "            preds = get_preds(z) #preds used for accuracy\n",
    "            \n",
    "            loss = loss_fcn(input=z, target=labels.float())\n",
    "\n",
    "            loss.backward() #get gradients \n",
    "\n",
    "            optimizer.step() #update parameters\n",
    "\n",
    "            train_acc = accuracy(preds,labels)\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            #add to overall records\n",
    "            train_acc_sum += train_acc\n",
    "            train_loss_sum += train_loss\n",
    "            \n",
    "            batch_count += 1\n",
    "\n",
    "        TrainAccRec.append(train_acc_sum/batch_count)\n",
    "        TrainLossRec.append(train_loss_sum/batch_count)\n",
    "        val_acc, val_loss = validate(model, valid_loader,loss_fcn)\n",
    "        ValAccRec.append(val_acc)\n",
    "        ValLossRec.append(val_loss)\n",
    "        \n",
    "        print(\"Epoch:\",epoch+1)\n",
    "        print(\"train acc:\",train_acc_sum/batch_count)\n",
    "        print(\"val acc:\",val_acc)\n",
    "        print(\"train loss:\", train_loss_sum/batch_count)\n",
    "        print(\"val loss:\", val_loss)\n",
    "        print(\"final output:\", z)\n",
    "        \n",
    "\n",
    "    toc = time()\n",
    "    print(\"Total training time:\", toc-tic)\n",
    "\n",
    "    test_acc, test_loss = validate(model,test_loader,loss_fcn)\n",
    "    \n",
    "    #plottting\n",
    "    e = np.arange(0,num_epochs)\n",
    "    plot(e,TrainAccRec,ValAccRec,'Epochs','Accuracy')\n",
    "    plot(e,TrainLossRec,ValLossRec,'Epochs','Losses')\n",
    "    print(\"Max training accuracy\",max(TrainAccRec))\n",
    "    print(\"Min training loss\",min(TrainLossRec),\"\\n\")\n",
    "    print(\"Max Validation accuracy\",max(ValAccRec))\n",
    "    print(\"Min Validation loss\",min(ValLossRec),\"\\n\")\n",
    "    print(\"Test accuracy\",test_acc)\n",
    "    print(\"Test loss\",test_loss,\"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "train(rseed,lr,num_epochs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kJlCum_EBNLh",
    "WbQCrISXWN46"
   ],
   "name": "MatchPredictor_MLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
